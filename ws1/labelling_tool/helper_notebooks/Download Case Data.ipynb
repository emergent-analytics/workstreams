{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download case data\n",
    "\n",
    "This notebook downloads COVID-19 case data from various sources and stores them in a generic triple store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "import numpy as np\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine\n",
    "import os\n",
    "try:\n",
    "    from project_lib import Project\n",
    "    CLOUDPAK = True\n",
    "except ModuleNotFoundError:\n",
    "    class Project():\n",
    "        def __init__(self):\n",
    "            pass\n",
    "        def get_name(self):\n",
    "            return \"Download case data\"\n",
    "    CLOUDPAK = False\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CLOUDPAK:\n",
    "    project = Project.access()\n",
    "    autos_credentials = project.get_connection(name=\"db2 Warehouse ealuser\")\n",
    "    sql_url = \"db2+ibm_db://{username}:{password}@{host}:{port}/{database};Security=ssl;\".format(**autos_credentials)\n",
    "elif \"SQL_CONNECT\" not in list(os.environ.keys()):\n",
    "    sql_url = \"sqlite:///database.sqlite\" # in case you want to run local\n",
    "    sql_url = \"postgresql://cookiecutter:cookiecutter@localhost:15432/cookiec\"\n",
    "else:\n",
    "    sql_url = os.environ[\"SQL_CONNECT\"]\n",
    "conn = create_engine(sql_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Waves from detecting changepoints\n",
    "\n",
    "This is a simple change point detector that determines if (infection) numbers were increasing for equal or more `THRESHOLD_UP` (14) days, or decreasing for more than `THRESHOLD_DOWN` (28) days, it then takes the first datetime of that period as a starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_log_entry(conn,notebook_name=\"data-ingestion/Download case data\",tablename=\"\",num_records_before=0,num_records_after=0,most_recent_datapoint=datetime.datetime(1980,1,1),error_code=0):\n",
    "    p = Project()\n",
    "    dfLogfile = pd.DataFrame({\"JOB_NAME\":[\"{}-{}\".format(p.get_name(),notebook_name)],\n",
    "                             \"NOTEBOOK_NAME\":[notebook_name],\n",
    "                             \"TABLE_NAME\":[tablename],\n",
    "                             \"DATETIME\":[datetime.datetime.now()],\n",
    "                             \"NUM_RECORDS\":[num_records_after],\n",
    "                             \"MOST_RECENT_DATETIME\":[most_recent_datapoint],\n",
    "                             \"RESULT\":error_code,\n",
    "                             \"MESSAGE\":[\"Number of Records before {} and after {}\".format(num_records_before,num_records_after)]})\n",
    "\n",
    "    dfLogfile.to_sql(\"cp4d_pipeline_status\", conn, if_exists='append',dtype={\"JOB_NAME\":sqlalchemy.types.String(150),\n",
    "                                                                             \"NOTEBOOK_NAME\":sqlalchemy.types.String(100),\n",
    "                                                                             \"TABLE_NAME\":sqlalchemy.types.String(100),\n",
    "                                                                             \"DATETIME\":sqlalchemy.types.DateTime,\n",
    "                                                                             \"MOST_RECENT_DATETIME\":sqlalchemy.types.DateTime,\n",
    "                                                                             \"MESSAGE\":sqlalchemy.types.String(200)},index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_waves(sCountry,identifier=\"\",country=\"\"):\n",
    "    # computing waves and \"periods of calmness\" using a very manual Schmitt-Trigger style detection of gradients up and down\n",
    "    all_verdicts = []\n",
    "    \n",
    "    THRESHOLD = 1\n",
    "    THRESHOLD_UP = 14\n",
    "    THRESHOLD_DOWN = 28\n",
    "    \n",
    "    data = sCountry.rolling(center=True,window=7).mean().dropna()\n",
    "        \n",
    "    datum = data.values[0]\n",
    "    increasing = 0\n",
    "    decreasing = 0\n",
    "    wave_no = 0\n",
    "    for i,v in data.items():\n",
    "        if v > datum:\n",
    "            if increasing == 0:\n",
    "                start_date = i\n",
    "            increasing += 1\n",
    "            if increasing > 3:\n",
    "                decreasing = 0\n",
    "        elif v < datum:\n",
    "            decreasing += 1\n",
    "            if decreasing > 3:\n",
    "                increasing = 0\n",
    "\n",
    "        if increasing == THRESHOLD_UP:\n",
    "            wave_no += 1\n",
    "            if len(all_verdicts)>0 and all_verdicts[-1][\"kind\"] == \"begin\":\n",
    "                pass\n",
    "            else:\n",
    "                all_verdicts.append({\"name\":country,\"datetime_date\":i,\"kind\":\"begin\",\"wave_no\":wave_no,\"identifier\":identifier})\n",
    "        if decreasing == THRESHOLD_DOWN:\n",
    "            if len(all_verdicts)>0 and all_verdicts[-1][\"kind\"] == \"end\":\n",
    "                all_verdicts.pop()\n",
    "                all_verdicts.append({\"name\":country,\"datetime_date\":i,\"kind\":\"end\",\"wave_no\":wave_no,\"identifier\":identifier})\n",
    "            else:\n",
    "                all_verdicts.append({\"name\":country,\"datetime_date\":i,\"kind\":\"end\",\"wave_no\":wave_no,\"identifier\":identifier})\n",
    "        datum = v\n",
    "\n",
    "    if len(all_verdicts) > 0:\n",
    "        dfWaves = pd.DataFrame(all_verdicts)\n",
    "        dfWaves = dfWaves.sort_values([\"name\",\"datetime_date\"])\n",
    "        return dfWaves\n",
    "    else:\n",
    "        return pd.DataFrame({\"name\":[],\"datetime_date\":[],\"kind\":[],\"wave_no\":[]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Johns Hopkins data\n",
    "\n",
    "This is a global dataset with certain deficiencies in that case data are not updated retrospectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfMapping = pd.read_sql(\"johns_hopkins_country_mapping\",conn)\n",
    "\n",
    "dfJH = pd.read_csv(\"https://github.com/CSSEGISandData/COVID-19/blob/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv?raw=true\",\n",
    "               encoding=\"utf-8\")\n",
    "\n",
    "dfJHcountries = dfJH[dfJH[\"Province/State\"].isnull()]\n",
    "dfJHcountries = dfJHcountries.merge(dfMapping,left_on=\"Country/Region\",right_on=\"name\")\n",
    "try:\n",
    "    dfJHcountries = dfJHcountries[dfJHcountries[\"ISO_3_code_i\"]>0]\n",
    "    del dfJHcountries[\"ISO_3_code_i\"]\n",
    "except:\n",
    "    dfJHcountries = dfJHcountries[dfJHcountries[\"iso_3_code_i\"]>0]\n",
    "    del dfJHcountries[\"iso_3_code_i\"]\n",
    "del dfJHcountries[\"Lat\"]\n",
    "del dfJHcountries[\"Long\"]\n",
    "del dfJHcountries[\"Province/State\"]\n",
    "del dfJHcountries[\"Country/Region\"]\n",
    "del dfJHcountries[\"adm0_a3\"]\n",
    "\n",
    "dfJHcountries.index = dfJHcountries.name\n",
    "dfJHcountries.index.name = None\n",
    "del dfJHcountries[\"name\"]\n",
    "dfJHcountries = dfJHcountries.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfJHcountries.index = pd.to_datetime(dfJHcountries.index)\n",
    "dfJHcountries = dfJHcountries.diff(1).dropna()\n",
    "\n",
    "dfJHcountries_trend = dfJHcountries.copy()\n",
    "for c in dfJHcountries_trend.columns:\n",
    "    try:\n",
    "        dfJHcountries_trend[c] = seasonal_decompose(dfJHcountries_trend[c],period=7).trend\n",
    "    except TypeError:\n",
    "        dfJHcountries_trend[c] = seasonal_decompose(dfJHcountries_trend[c],freq=7).trend\n",
    "\n",
    "dfnew_cases = dfJHcountries.stack().reset_index().rename(columns={\"level_0\":\"datetime_date\",\"level_1\":\"name\",0:\"new_cases\"})\n",
    "dftrend = dfJHcountries_trend.stack().reset_index().rename(columns={\"level_0\":\"datetime_date\",\"level_1\":\"name\",0:\"trend\"})\n",
    "df = pd.merge(dfnew_cases,dftrend,how=\"left\",left_on=[\"datetime_date\",\"name\"],right_on=[\"datetime_date\",\"name\"])\n",
    "df[\"new_cases\"] = df[\"new_cases\"].astype(int)\n",
    "df[\"data_source\"] = \"Johns Hopkins global\"\n",
    "\n",
    "df = df.merge(dfMapping,on=\"name\").rename(columns={\"adm0_a3\":\"identifier\"})\n",
    "#del df[\"ISO_3_code_i\"]\n",
    "\n",
    "try:\n",
    "    r = conn.execute(\"SELECT COUNT(*) FROM cookiecutter_case_data WHERE data_source='Johns Hopkins global'\")\n",
    "    num_records_before = int(r.fetchone()[0])\n",
    "    conn.execute(\"DELETE FROM cookiecutter_case_data WHERE data_source='Johns Hopkins global'\")\n",
    "    error_code = 0\n",
    "except:\n",
    "    num_records_before = 0\n",
    "    error_code = 1\n",
    "df.to_sql(\"cookiecutter_case_data\",conn,index=False,dtype={\"datetime_date\":sqlalchemy.types.DateTime,\n",
    "                                                          \"name\":sqlalchemy.types.VARCHAR(100),\n",
    "                                                           \"identifier\":sqlalchemy.types.VARCHAR(10),\n",
    "                                                           \"data_source\":sqlalchemy.types.VARCHAR(30)},\n",
    "         if_exists=\"append\")\n",
    "\n",
    "create_log_entry(conn,notebook_name=\"data-ingestion/Download case data\",\n",
    "                 tablename=\"cookiecutter_case_data/Johns Hopkins global\",\n",
    "                 num_records_before=num_records_before,num_records_after=len(df),\n",
    "                 most_recent_datapoint=df[\"datetime_date\"].max(),error_code=error_code)\n",
    "\n",
    "\n",
    "allwaves = []\n",
    "for c in dfJHcountries.columns:\n",
    "    allwaves.append(compute_waves(dfJHcountries[c],country=c,identifier=dfMapping[dfMapping.name == c].adm0_a3.unique()[0]))\n",
    "dfWaves = pd.DataFrame().append(allwaves,sort=True)\n",
    "dfWaves[\"wave_no\"] = dfWaves[\"wave_no\"].astype(int)\n",
    "dfWaves[\"data_source\"] = \"Johns Hopkins global\"\n",
    "try:\n",
    "    r = conn.execute(\"SELECT COUNT(*) FROM cookiecutter_computed_waves_chgpoint WHERE data_source='Johns Hopkins global'\")\n",
    "    num_records_before = int(r.fetchone()[0])\n",
    "    conn.execute(\"DELETE FROM cookiecutter_computed_waves_chgpoint WHERE data_source='Johns Hopkins global'\")\n",
    "    error_code = 0\n",
    "except:\n",
    "    num_records_before = 0\n",
    "    error_code = 1\n",
    "dfWaves.to_sql(\"cookiecutter_computed_waves_chgpoint\",conn,index=False,dtype={\"name\":sqlalchemy.types.VARCHAR(100),\n",
    "                                                                             \"datetime_date\":sqlalchemy.types.DateTime,\n",
    "                                                                             \"kind\":sqlalchemy.types.VARCHAR(10),\n",
    "                                                                             \"identifier\":sqlalchemy.types.VARCHAR(10),\n",
    "                                                                             \"data_source\":sqlalchemy.types.VARCHAR(30)},\n",
    "         if_exists=\"append\")\n",
    "\n",
    "create_log_entry(conn,notebook_name=\"data-ingestion/Download case data\",\n",
    "                 tablename=\"cookiecutter_computed_waves_chgpoint/Johns Hopkins global\",\n",
    "                 num_records_before=num_records_before,num_records_after=len(dfWaves),\n",
    "                 most_recent_datapoint=df[\"datetime_date\"].max(),error_code=error_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Johns Hopkins US Data\n",
    "\n",
    "The data are down to Counties but we aggregate them to States."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfJHUS = pd.read_csv(\"https://github.com/CSSEGISandData/COVID-19/blob/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_US.csv?raw=true\",\n",
    "               encoding=\"utf-8\")\n",
    "dfJHUS = dfJHUS[dfJHUS.FIPS.notnull()]\n",
    "\n",
    "state_to_postal = dict(zip(['Alabama','Alaska','Arizona','Arkansas','California','Colorado','Connecticut','Delaware','Florida','Georgia',\n",
    "          'Hawaii','Idaho','Illinois','Indiana','Iowa','Kansas','Kentucky','Louisiana','Maine','Maryland','Massachusetts',\n",
    "          'Michigan','Minnesota','Mississippi','Missouri','Montana','Nebraska','Nevada','New Hampshire','New Jersey',\n",
    "          'New Mexico','New York','North Carolina','North Dakota','Ohio','Oklahoma','Oregon','Pennsylvania','Rhode Island',\n",
    "          'South Carolina','South Dakota','Tennessee','Texas','Utah','Vermont','Virginia','Washington','West Virginia',\n",
    "          'Wisconsin','Wyoming','American Samoa','Guam','Northern Mariana Islands','Puerto Rico','Virgin Islands'],\n",
    "         ['AL','AK','AZ','AR','CA','CO','CT','DE','FL','GA','HI','ID','IL','IN','IA','KS','KY','LA','ME','MD',\n",
    "          'MA','MI','MN','MS','MO','MT','NE','NV','NH','NJ','NM','NY','NC','ND','OH','OK','OR','PA','RI','SC',\n",
    "          'SD','TN','TX','UT','VT','VA','WA','WV','WI','WY','AS','GU','MP','PR','VI']))\n",
    "\n",
    "dfJHUS[\"name\"] = \"\"\n",
    "for i,row in dfJHUS.iterrows():\n",
    "    try:\n",
    "        dfJHUS.at[i,\"name\"] = \"US-\"+state_to_postal[row[\"Province_State\"]]+\" \"+row[\"Province_State\"]\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "dfJHUS = dfJHUS[dfJHUS.name > \"\"].groupby(\"name\").sum()\n",
    "del dfJHUS[\"UID\"]\n",
    "del dfJHUS[\"code3\"]\n",
    "del dfJHUS[\"FIPS\"]\n",
    "del dfJHUS[\"Lat\"]\n",
    "del dfJHUS[\"Long_\"]\n",
    "dfJHUS.index.name = None\n",
    "dfJHUS = dfJHUS.transpose()\n",
    "dfJHUS.index = pd.to_datetime(dfJHUS.index)\n",
    "dfJHUS = dfJHUS.diff(1).dropna()\n",
    "\n",
    "dfnew_cases = dfJHUS.stack().reset_index().rename(columns={\"level_0\":\"datetime_date\",\"level_1\":\"name\",0:\"new_cases\"})\n",
    "\n",
    "dfJHUS_trend = dfJHUS.copy()\n",
    "for c in dfJHUS_trend.columns:\n",
    "    try:\n",
    "        dfJHUS_trend[c] = seasonal_decompose(dfJHUS_trend[c],period=7).trend\n",
    "    except TypeError:\n",
    "        dfJHUS_trend[c] = seasonal_decompose(dfJHUS_trend[c],freq=7).trend\n",
    "\n",
    "dftrend = dfJHUS_trend.stack().reset_index().rename(columns={\"level_0\":\"datetime_date\",\"level_1\":\"name\",0:\"trend\"})\n",
    "\n",
    "df = pd.merge(dfnew_cases,dftrend,how=\"left\",left_on=[\"datetime_date\",\"name\"],right_on=[\"datetime_date\",\"name\"])\n",
    "df[\"new_cases\"] = df[\"new_cases\"].astype(int)\n",
    "df[\"data_source\"] = \"Johns Hopkins US States\"\n",
    "\n",
    "dfCodeMapping = df.name.str.split(expand=True).rename(columns={0:\"identifier\"})\n",
    "del dfCodeMapping[1]\n",
    "del dfCodeMapping[2]\n",
    "del dfCodeMapping[3]\n",
    "df = df.join(dfCodeMapping)\n",
    "\n",
    "try:\n",
    "    r = conn.execute(\"SELECT COUNT(*) FROM cookiecutter_case_data WHERE data_source='Johns Hopkins US States'\")\n",
    "    num_records_before = int(r.fetchone()[0])\n",
    "    conn.execute(\"DELETE FROM cookiecutter_case_data WHERE data_source='Johns Hopkins US States'\")\n",
    "    error_code = 0\n",
    "except:\n",
    "    num_records_before = 0\n",
    "    error_code = 1\n",
    "    \n",
    "df.to_sql(\"cookiecutter_case_data\",conn,index=False,dtype={\"datetime_date\":sqlalchemy.types.DateTime,\n",
    "                                                          \"name\":sqlalchemy.types.VARCHAR(100),\n",
    "                                                          \"identifier\":sqlalchemy.types.VARCHAR(10),\n",
    "                                                           \"data_source\":sqlalchemy.types.VARCHAR(30)},\n",
    "         if_exists=\"append\")\n",
    "create_log_entry(conn,notebook_name=\"data-ingestion/Download case data\",\n",
    "                 tablename=\"cookiecutter_case_data/Johns Hopkins US States\",\n",
    "                 num_records_before=num_records_before,num_records_after=len(df),\n",
    "                 most_recent_datapoint=df[\"datetime_date\"].max(),error_code=error_code)\n",
    "\n",
    "allwaves = []\n",
    "for c in dfJHUS.columns:\n",
    "    allwaves.append(compute_waves(dfJHUS[c],country=c,identifier=c.split(\" \")[0]))\n",
    "dfWaves = pd.DataFrame().append(allwaves,sort=True)\n",
    "dfWaves[\"wave_no\"] = dfWaves[\"wave_no\"].astype(int)\n",
    "dfWaves[\"data_source\"] = \"Johns Hopkins US States\"\n",
    "try:\n",
    "    r = conn.execute(\"SELECT COUNT(*) FROM cookiecutter_computed_waves_chgpoint WHERE data_source='Johns Hopkins US States'\")\n",
    "    num_records_before = int(r.fetchone()[0])\n",
    "    conn.execute(\"DELETE FROM cookiecutter_computed_waves_chgpoint WHERE data_source='Johns Hopkins US States'\")\n",
    "    error_code = 0\n",
    "except:\n",
    "    num_records_before = 0\n",
    "    error_code = 1\n",
    "dfWaves.to_sql(\"cookiecutter_computed_waves_chgpoint\",conn,index=False,dtype={\"name\":sqlalchemy.types.VARCHAR(100),\n",
    "                                                                             \"datetime_date\":sqlalchemy.types.DateTime,\n",
    "                                                                             \"kind\":sqlalchemy.types.VARCHAR(10),\n",
    "                                                                             \"identifier\":sqlalchemy.types.VARCHAR(10),\n",
    "                                                                             \"data_source\":sqlalchemy.types.VARCHAR(30)},\n",
    "         if_exists=\"append\")\n",
    "create_log_entry(conn,notebook_name=\"data-ingestion/Download case data\",\n",
    "                 tablename=\"cookiecutter_computed_waves_chgpoint/Johns Hopkins US States\",\n",
    "                 num_records_before=num_records_before,num_records_after=len(dfWaves),\n",
    "                 most_recent_datapoint=df[\"datetime_date\"].max(),error_code=error_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## European Centre of Disease Control (ECDC) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfECDC = pd.read_csv(\"https://opendata.ecdc.europa.eu/covid19/casedistribution/csv/data.csv\",encoding=\"utf-8\")\n",
    "\n",
    "ddf = dfECDC[[\"countriesAndTerritories\",\"countryterritoryCode\"]].drop_duplicates()\n",
    "ecdc_countries_to_adm0_a3 = dict(zip(ddf.countriesAndTerritories.values,ddf.countryterritoryCode.values))\n",
    "\n",
    "dfECDC.index = pd.to_datetime(dfECDC.dateRep,format=\"%d/%m/%Y\")\n",
    "dfECDC.index.name = None\n",
    "dfECDC = dfECDC[[\"cases\",\"countriesAndTerritories\"]]\n",
    "dfECDC = pd.pivot_table(dfECDC,columns=[\"countriesAndTerritories\"],values=\"cases\",index=dfECDC.index).fillna(0)\n",
    "\n",
    "dfECDC_trend = dfECDC.copy()\n",
    "\n",
    "for c in dfECDC_trend.columns:\n",
    "    try:\n",
    "        dfECDC_trend[c] = seasonal_decompose(dfECDC_trend[c],period=7).trend\n",
    "    except TypeError:\n",
    "        dfECDC_trend[c] = seasonal_decompose(dfECDC_trend[c],freq=7).trend\n",
    "\n",
    "dfnew_cases = dfECDC.stack().reset_index().rename(columns={\"level_0\":\"datetime_date\",\"level_1\":\"name\",0:\"new_cases\",\"countriesAndTerritories\":\"name\"})\n",
    "dftrend = dfECDC_trend.stack().reset_index().rename(columns={\"level_0\":\"datetime_date\",\"level_1\":\"name\",0:\"trend\",\"countriesAndTerritories\":\"name\"})\n",
    "df = pd.merge(dfnew_cases,dftrend,how=\"left\",left_on=[\"datetime_date\",\"name\"],right_on=[\"datetime_date\",\"name\"])\n",
    "df[\"new_cases\"] = df[\"new_cases\"].astype(int)\n",
    "df[\"data_source\"] = \"ECDC global\"\n",
    "\n",
    "try:\n",
    "    r = conn.execute(\"SELECT COUNT(*) FROM cookiecutter_case_data WHERE data_source='ECDC global'\")\n",
    "    num_records_before = int(r.fetchone()[0])\n",
    "    conn.execute(\"DELETE FROM cookiecutter_case_data WHERE data_source='ECDC global'\")\n",
    "    error_code = 0\n",
    "except:\n",
    "    num_records_before = 0\n",
    "    error_code = 1\n",
    "    \n",
    "df = df.merge(ddf,left_on=\"name\",right_on=\"countriesAndTerritories\").rename(columns={\"countryterritoryCode\":\"identifier\"})\n",
    "del df[\"countriesAndTerritories\"]\n",
    "df.to_sql(\"cookiecutter_case_data\",conn,index=False,dtype={\"datetime_date\":sqlalchemy.types.DateTime,\n",
    "                                                          \"name\":sqlalchemy.types.VARCHAR(100),\n",
    "                                                           \"identifier\":sqlalchemy.types.VARCHAR(10),\n",
    "                                                           \"data_source\":sqlalchemy.types.VARCHAR(30)},\n",
    "         if_exists=\"append\")\n",
    "create_log_entry(conn,notebook_name=\"data-ingestion/Download case data\",\n",
    "                 tablename=\"cookiecutter_case_data/ECDC global\",\n",
    "                 num_records_before=num_records_before,num_records_after=len(df),\n",
    "                 most_recent_datapoint=df[\"datetime_date\"].max(),error_code=error_code)\n",
    "\n",
    "allwaves = []\n",
    "for c in dfECDC.columns:\n",
    "    allwaves.append(compute_waves(dfECDC[c],country=c,identifier=ecdc_countries_to_adm0_a3[c]))\n",
    "dfWaves = pd.DataFrame().append(allwaves,sort=True)\n",
    "dfWaves[\"wave_no\"] = dfWaves[\"wave_no\"].astype(int)\n",
    "dfWaves[\"data_source\"] = \"ECDC global\"\n",
    "\n",
    "try:\n",
    "    r = conn.execute(\"SELECT COUNT(*) FROM cookiecutter_computed_waves_chgpoint WHERE data_source='ECDC global'\")\n",
    "    num_records_before = int(r.fetchone()[0])\n",
    "    conn.execute(\"DELETE FROM cookiecutter_computed_waves_chgpoint WHERE data_source='ECDC global'\")\n",
    "    error_code = 0\n",
    "except:\n",
    "    num_records_before = 0\n",
    "    error_code = 1\n",
    "    \n",
    "dfWaves.to_sql(\"cookiecutter_computed_waves_chgpoint\",conn,index=False,dtype={\"name\":sqlalchemy.types.VARCHAR(100),\n",
    "                                                                             \"datetime_date\":sqlalchemy.types.DateTime,\n",
    "                                                                             \"kind\":sqlalchemy.types.VARCHAR(10),\n",
    "                                                                             \"identifier\":sqlalchemy.types.VARCHAR(10),\n",
    "                                                                             \"data_source\":sqlalchemy.types.VARCHAR(30)},\n",
    "         if_exists=\"append\")\n",
    "create_log_entry(conn,notebook_name=\"data-ingestion/Download case data\",\n",
    "                 tablename=\"cookiecutter_computed_waves_chgpoint/ECDC global\",\n",
    "                 num_records_before=num_records_before,num_records_after=len(dfWaves),\n",
    "                 most_recent_datapoint=df[\"datetime_date\"].max(),error_code=error_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Germany Robert-Koch Institut (RKI) Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "bundesland_code = {'Baden-Württemberg': 'DE-BW', 'Bayern': 'DE-BY', 'Berlin': 'DE-BE', 'Brandenburg': 'DE-BB', 'Bremen': 'DE-HB',\n",
    "                   'Hamburg': 'DE-HH', 'Hessen': 'DE-HE', 'Mecklenburg-Vorpommern': 'DE-MV', 'Niedersachsen': 'DE-NI',\n",
    "                   'Nordrhein-Westfalen': 'DE-NW', 'Rheinland-Pfalz': 'DE-RP', 'Saarland': 'DE-SL', 'Sachsen': 'DE-SN',\n",
    "                   'Sachsen-Anhalt': 'DE-ST', 'Schleswig-Holstein': 'DE-SH', 'Thüringen': 'DE-TH'}\n",
    "\n",
    "dfRKI = pd.read_csv(\"https://www.arcgis.com/sharing/rest/content/items/f10774f1c63e40168479a1feb6c7ca74/data\",encoding=\"utf-8\")\n",
    "dfRKI.index = pd.to_datetime(dfRKI.Refdatum)\n",
    "dfRKI[\"datetime_date\"] = dfRKI.index\n",
    "\n",
    "dfRKI = dfRKI[[\"Bundesland\",\"datetime_date\",\"AnzahlFall\"]].groupby([\"Bundesland\",\"datetime_date\"]).sum().reset_index().fillna(0)\n",
    "dfRKI = pd.pivot_table(dfRKI,columns=[\"Bundesland\"],index=[\"datetime_date\"],values=\"AnzahlFall\").fillna(0)\n",
    "\n",
    "dfRKI_trend = dfRKI.copy()\n",
    "\n",
    "for c in dfRKI_trend.columns:\n",
    "    try:\n",
    "        dfRKI_trend[c] = seasonal_decompose(dfRKI_trend[c],period=7).trend\n",
    "    except TypeError:\n",
    "        dfRKI_trend[c] = seasonal_decompose(dfRKI_trend[c],freq=7).trend\n",
    "\n",
    "dfnew_cases = dfRKI.stack().reset_index().rename(columns={\"level_0\":\"datetime_date\",\"level_1\":\"name\",0:\"new_cases\",\"countriesAndTerritories\":\"name\",\"Bundesland\":\"name\"})\n",
    "dftrend = dfRKI_trend.stack().reset_index().rename(columns={\"level_0\":\"datetime_date\",\"level_1\":\"name\",0:\"trend\",\"countriesAndTerritories\":\"name\",\"Bundesland\":\"name\"})\n",
    "\n",
    "df = pd.merge(dfnew_cases,dftrend,how=\"left\",left_on=[\"datetime_date\",\"name\"],right_on=[\"datetime_date\",\"name\"])\n",
    "df[\"new_cases\"] = df[\"new_cases\"].astype(int)\n",
    "df[\"data_source\"] = \"RKI D\"\n",
    "dfBundeslandMapping = pd.DataFrame(zip(bundesland_code.keys(),bundesland_code.values())).rename(columns={0:\"name\",1:\"identifier\"})\n",
    "df = df.merge(dfBundeslandMapping,on=\"name\")\n",
    "\n",
    "try:\n",
    "    r = conn.execute(\"SELECT COUNT(*) FROM cookiecutter_case_data WHERE data_source='RKI D'\")\n",
    "    num_records_before = int(r.fetchone()[0])\n",
    "    conn.execute(\"DELETE FROM cookiecutter_case_data WHERE data_source='RKI D'\")\n",
    "    error_code = 0\n",
    "except:\n",
    "    num_records_before = 0\n",
    "    error_code = 1\n",
    "df.to_sql(\"cookiecutter_case_data\",conn,index=False,dtype={\"datetime_date\":sqlalchemy.types.DateTime,\n",
    "                                                           \"name\":sqlalchemy.types.VARCHAR(100),\n",
    "                                                           \"identifier\":sqlalchemy.types.VARCHAR(10),\n",
    "                                                           \"data_source\":sqlalchemy.types.VARCHAR(30)},\n",
    "          if_exists=\"append\")\n",
    "create_log_entry(conn,notebook_name=\"data-ingestion/Download case data\",\n",
    "                 tablename=\"cookiecutter_case_data/RKI D\",\n",
    "                 num_records_before=num_records_before,num_records_after=len(df),\n",
    "                 most_recent_datapoint=df[\"datetime_date\"].max(),error_code=error_code)\n",
    "\n",
    "\n",
    "allwaves = []\n",
    "for c in dfRKI.columns:\n",
    "    allwaves.append(compute_waves(dfRKI[c],country=c,identifier=bundesland_code[c]))\n",
    "dfWaves = pd.DataFrame().append(allwaves,sort=True)\n",
    "dfWaves[\"wave_no\"] = dfWaves[\"wave_no\"].astype(int)\n",
    "dfWaves[\"data_source\"] = \"RKI D\"\n",
    "\n",
    "try:\n",
    "    r = conn.execute(\"SELECT COUNT(*) FROM cookiecutter_computed_waves_chgpoint WHERE data_source='RKI D'\")\n",
    "    num_records_before = int(r.fetchone()[0])\n",
    "    conn.execute(\"DELETE FROM cookiecutter_computed_waves_chgpoint WHERE data_source='RKI D'\")\n",
    "    error_code = 0\n",
    "except:\n",
    "    num_records_before = 0\n",
    "    error_code = 1\n",
    "dfWaves.to_sql(\"cookiecutter_computed_waves_chgpoint\",conn,index=False,dtype={\"name\":sqlalchemy.types.VARCHAR(100),\n",
    "                                                                             \"datetime_date\":sqlalchemy.types.DateTime,\n",
    "                                                                             \"kind\":sqlalchemy.types.VARCHAR(10),\n",
    "                                                                             \"identifier\":sqlalchemy.types.VARCHAR(10),\n",
    "                                                                             \"data_source\":sqlalchemy.types.VARCHAR(30)},\n",
    "         if_exists=\"append\")\n",
    "create_log_entry(conn,notebook_name=\"data-ingestion/Download case data\",\n",
    "                 tablename=\"cookiecutter_computed_waves_chgpoint/RKI D\",\n",
    "                 num_records_before=num_records_before,num_records_after=len(dfWaves),\n",
    "                 most_recent_datapoint=df[\"datetime_date\"].max(),error_code=error_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
