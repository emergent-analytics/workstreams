{"cells": [{"metadata": {"collapsed": true}, "cell_type": "markdown", "source": "## Topic Modelling"}, {"metadata": {}, "cell_type": "markdown", "source": "The goal of this notebook is to find the topics on which people are talking within our dataset with tweets about vaccines. There are many models available for topic modelling, but in this Notebook we've focused only on **LDA (Latent Dirichlet Allocation)**.\n\n\n#### Input\n- A dataset with tweets ready to be used by our LDA algorithm: `vacc_proc_for_topicMdl.csv`\n\n#### Output\n- An html where we can visualise the discovered topics: `Vaccs_Notts_topic_7.html`\n- A dataset with tweets mapped to their main topic: `topics_mapped_Vaccs_Notts.csv`"}, {"metadata": {}, "cell_type": "code", "source": "# ----------------------------------------\n# Libraries need to be installed\n# ----------------------------------------\n\n!pip install pyLDAvis\n!pip install gensim\n!pip install spacy\n!python -m spacy download en_core_web_sm\n\n\n# ----------------------------------------    \n# For File operations\n# ----------------------------------------\n\nimport zipfile\nimport os\n\n# ----------------------------------------\n# Data read, write and other operations on Texts\n# ----------------------------------------\n\nimport pandas as pd\nimport numpy as np\nimport string\nimport re\nimport unicodedata\nfrom pprint import pprint\n\n# ----------------------------------------\n# For Libaries for NLP applications\n# ----------------------------------------\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom nltk.tokenize import word_tokenize\nfrom nltk.probability import FreqDist\nimport gensim\nimport spacy\nspcy = spacy.load('/opt/conda/envs/Python-3.6-WMLCE/lib/python3.6/site-packages/en_core_web_sm/en_core_web_sm-2.3.1')\nfrom gensim import corpora\nfrom gensim.models import CoherenceModel\n\n# ----------------------------------------\n# For ignoring some warnings\n# ----------------------------------------\n\nimport warnings\nwarnings.filterwarnings('ignore')\ndef wrng():\n  warnings.warn(\"deprecated\", DeprecationWarning)\n\nwith warnings.catch_warnings():\n  warnings.simplefilter(\"ignore\")\n  wrng()\n\n# ----------------------------------------    \n# For Visualizations\n# ----------------------------------------\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport pyLDAvis\nimport pyLDAvis.gensim as pygen\npyLDAvis.enable_notebook()\n\n# ----------------------------------------    \n# Need to download some extras\n# ----------------------------------------\n\nnltk.download('punkt')\nnltk.download('stopwords')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Load Dataset\nHere the datset is used from the `TopicModelling_Vaccine_Preprocessing` notebook."}, {"metadata": {}, "cell_type": "code", "source": "processed_tweets_Vaccs_ = pd.read_csv(\"/project_data/data_asset/vacc_proc_for_topicMdl.csv\")\npd.set_option('display.max_columns', None)  # Showing all columns for that dataframe", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Filtering data related to 'Nottingham' "}, {"metadata": {}, "cell_type": "code", "source": "notts_tweets_Vaccs_ = processed_tweets_Vaccs_[processed_tweets_Vaccs_[\"City\"] == \"Nottingham\"]", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Part-of-Speech tagging\n Filtering words based on particular part-of-speech as other parts of speech could generate noise for topics"}, {"metadata": {}, "cell_type": "code", "source": "sentences = []\nfor line in notts_tweets_Vaccs_[\"Clean_sentence_Comment\"]:\n    pos_ = spcy(line)\n\n    sentence2 = \" \".join([token.text for token in pos_ if (token.pos_ == \"ADJ\" or token.pos_ == \"NOUN\" or token.pos_ == \"PROPN\" or token.pos_ == \"VERB\")])\n    sentences.append(sentence2)\n    \nnotts_tweets_Vaccs_[\"Clean_sentence_Comment\"] = sentences", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Filtering words\nFiltering the least and most frequent words (filters if less than 'no_below', more than 'no_above')"}, {"metadata": {}, "cell_type": "code", "source": "words = [text.split() for text in notts_tweets_Vaccs_[\"Clean_sentence_Comment\"]]\ndict_words = corpora.Dictionary(words)\ndict_words.filter_extremes(no_below=5, no_above=0.2) \ndict_words.compactify()\nmyCorpus_notts = [dict_words.doc2bow(word) for word in words]", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Training LDA Model\nHere we train the LDA model and compute the coherence metric and log-perplexity for a range of topic numbers and other hyperparameters. Here, we've focused on coherence metric to choose the best model."}, {"metadata": {}, "cell_type": "code", "source": "\nMulLda_coherent_scores = []\nMulLda_topics_val = []\nMulLda_perplexity_val = []\nalpha_val = [0.05, 0.1, 0.3, 0.5, 0.8, 1]\nMulLda_alphas = []\n\n\nfor topics in range(3, 15, 2):\n        for alph in alpha_val:\n\n            lda_model_multi_notts = gensim.models.LdaMulticore(corpus = myCorpus_notts,\n                         id2word = dict_words,\n                         random_state = 42,\n                         num_topics = topics,\n                         passes=10,\n                         chunksize=512,\n                         alpha=alph,\n                         offset=64,\n                         eta=None,\n                         iterations=100,\n                         per_word_topics=True,\n                         workers=6)\n  \n            coherence_model_MulLda_notts = CoherenceModel(model = lda_model_multi_notts, \n                                       texts = words, \n                                       dictionary = dict_words, \n                                       coherence = 'c_v')\n  \n            coherence_MulLda = coherence_model_MulLda_notts.get_coherence()\n            perplexity_MulLda = lda_model_multi_notts.log_perplexity(myCorpus_notts)\n        \n            MulLda_topics_val.append(topics)\n            MulLda_alphas.append(alph)        \n            MulLda_coherent_scores.append(coherence_MulLda)\n            MulLda_perplexity_val.append(perplexity_MulLda)\n            \n\n            \ndf_mulLDA_notts = pd.DataFrame(list(zip(MulLda_topics_val, MulLda_alphas, MulLda_coherent_scores, MulLda_perplexity_val)), \n                         columns = [\"MulLda_Topic_Num\", \"MulLda_alpha_val\", \"MulLda_Coherent_score\", \"MulLda_Perplexity_val\"])\n\ndf_mulLDA_notts.sort_values(\"MulLda_Coherent_score\", axis = 0, ascending = False, \n                     inplace = True) \n\ndf_mulLDA_notts.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Final Model\nAfter choosing best hyperparams from above dataframe based on coherence metric, we can train our final model. Note that we haven't just fully relied on the highest value for this metric, but we have rather chosen the model that makes the most sense based in our experience from the top models.   \n\nThe cell below will output the words related to some topics and clusters of topics (visualization)."}, {"metadata": {}, "cell_type": "code", "source": "\nmulti_lda_final_notts = gensim.models.LdaMulticore(corpus = myCorpus_notts,\n                         id2word = dict_words,\n                         random_state = 42,\n                         num_topics = 7,\n                         passes=10,\n                         chunksize=512,\n                         alpha=0.05,\n                         offset=64,\n                         eta=None,\n                         iterations=100,\n                         per_word_topics=True,\n                         workers=6)\n\npprint(multi_lda_final_notts.print_topics(num_topics = 7, num_words=20))\n\nprint(\"\\n\\033[91m\" + \"\\033[1m\" +\"------- Visualization -----------\\n\")\n\nlda_Mul_vis_notts = pygen.prepare(multi_lda_final_notts, myCorpus_notts, dict_words)\npyLDAvis.display(lda_Mul_vis_notts)\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Saving Topics as html"}, {"metadata": {}, "cell_type": "code", "source": "pyLDAvis.save_html(lda_Mul_vis_notts, \"/project_data/data_asset/Vaccs_Notts_topic_7.html\")", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Mapping Tweets with Topics"}, {"metadata": {}, "cell_type": "code", "source": "\ntopicss = []\nprobss = []\n\nfor i, row in enumerate(multi_lda_final_notts[myCorpus_notts]):     # gives topics probablity\n\n    row = sorted(row[0], key=lambda x :(x[1]), reverse=True)    # sorting according to higher probability\n    for j, (topic_num, probablity) in enumerate(row):        # j=0  --> containing highest probablity, topic_num --> falls under which topic\n        if j == 0:\n            topicss.append(topic_num)\n            probss.append(probablity)\n            \nNotts_tweets_Vaccs_[\"Topic_Num\"] = topicss\nNotts_tweets_Vaccs_[\"Topic_prob\"] = probss\n\nNotts_tweets_Vaccs_.head()\n\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Final Dataset\nwe've given the topics some names and mapped with the tweets"}, {"metadata": {}, "cell_type": "code", "source": "\"\"\"\n\nlist_ - values of list needs to converted to string\n\n\"\"\"\ndef ListToStr(list_):\n  str_val = \"\"\n  for item in list_:\n    str_val += item\n  return str_val\n\ndts = []\n\nfor dttt in Notts_tweets_Vaccs_[\"Date\"]:\n    yrs_ = re.findall(r\"\\d{4}\", dttt)\n    dts.append(ListToStr(yrs_))\n    \nNotts_tweets_Vaccs_[\"year\"] = dts\n\nNotts_tweets_Vaccs_[\"Date\"] = pd.to_datetime(Notts_tweets_Vaccs_[\"Date\"]).dt.date\n\ntpc_nms = []\n\nfor tpc_ in Notts_tweets_Vaccs_[\"Topic_Num\"].values.tolist():\n    if tpc_ == 0:\n        tpc_nms.append(\"Effects of virus and vaccine\")\n    if tpc_ == 1:\n        tpc_nms.append(\"Politics in US around vaccine\")\n    if tpc_ == 2:\n        tpc_nms.append(\"Enforcement of vaccines\")\n    if tpc_ == 3:\n        tpc_nms.append(\"Politics in UK around vaccine\")\n    if tpc_ == 4:\n        tpc_nms.append(\"Science around Vaccine\")\n    if tpc_ == 5:\n        tpc_nms.append(\"Public affairs\")\n    if tpc_ == 6:\n        tpc_nms.append(\"Distribution of vaccine and logistics\")\n        \nNotts_tweets_Vaccs_[\"Topic_Names\"] = tpc_nms\n\n\ntyms = []\n\nfor tym in Notts_tweets_Vaccs_[\"Date\"].values.tolist():\n    tym_ = tym.strftime('%d-%b')\n    tyms.append(tym_)\n    \nNotts_tweets_Vaccs_[\"Date_month\"] = tyms\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Saving Final dataset"}, {"metadata": {}, "cell_type": "code", "source": "Notts_tweets_Vaccs_.to_csv('/project_data/data_asset/topics_mapped_Vaccs_Notts.csv', index = False)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "  \n  \n### Author:"}, {"metadata": {}, "cell_type": "markdown", "source": "-  **Ananda Pal** is a Data Scientist and Performance Test Analyst at IBM, where he specialises in Data Science and Machine Learning Solutions"}, {"metadata": {}, "cell_type": "markdown", "source": "Copyright \u00a9 IBM Corp. 2020. Licensed under the Apache License, Version 2.0. Released as licensed Sample Materials."}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.6", "language": "python"}, "language_info": {"name": "python", "version": "3.6.10", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 1}